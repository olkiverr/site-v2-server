import requests
import mysql.connector
import time
import json
import argparse
import sys
import os
import logging
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
import hashlib

# Configurer le syst√®me de logs
class Logger:
    def __init__(self, log_dir="logs", log_level=logging.INFO):
        self.terminal = sys.stdout
        
        # Cr√©er le r√©pertoire de logs s'il n'existe pas
        os.makedirs(log_dir, exist_ok=True)
        
        # Configurer le logger
        self.logger = logging.getLogger("MangamuseImporter")
        self.logger.setLevel(log_level)
        
        # D√©finir le format des logs
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        
        # Cr√©er un fichier de log avec la date du jour
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        log_filename = os.path.join(log_dir, f"import_{timestamp}.log")
        
        # Configurer le handler de fichier
        file_handler = logging.FileHandler(log_filename, encoding='utf-8')
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)
        
        print(f"Les logs seront enregistr√©s dans: {log_filename}")
    
    def log(self, message, level=logging.INFO):
        """Enregistre un message dans le fichier de logs et l'affiche aussi dans le terminal"""
        # Afficher dans le terminal
        print(message)
        
        # Enregistrer dans le fichier de logs
        if level == logging.DEBUG:
            self.logger.debug(message)
        elif level == logging.INFO:
            self.logger.info(message)
        elif level == logging.WARNING:
            self.logger.warning(message)
        elif level == logging.ERROR:
            self.logger.error(message)
        elif level == logging.CRITICAL:
            self.logger.critical(message)

def parse_arguments():
    parser = argparse.ArgumentParser(description="Ajouter des anim√©s √† la base de donn√©es MangaMuse")
    parser.add_argument("--start", type=int, default=1, help="Page de d√©part (par d√©faut: 1)")
    parser.add_argument("--end", type=int, help="Page de fin (optionnel, va jusqu'√† la derni√®re page disponible si non sp√©cifi√©)")
    parser.add_argument("--filter-adult", action="store_true", help="Activer le filtre pour exclure le contenu adulte (Ecchi, Erotica, Hentai)")
    parser.add_argument("--no-filter", action="store_true", help="D√©sactiver tous les filtres de contenu")
    parser.add_argument("--db-host", default="localhost", help="H√¥te de la base de donn√©es (par d√©faut: localhost)")
    parser.add_argument("--db-user", default="root", help="Utilisateur de la base de donn√©es (par d√©faut: root)")
    parser.add_argument("--db-password", default="", help="Mot de passe de la base de donn√©es (par d√©faut: vide)")
    parser.add_argument("--db-name", default="mangamuse", help="Nom de la base de donn√©es (par d√©faut: mangamuse)")
    parser.add_argument("--skip-details", action="store_true", help="Ignorer la recherche des d√©tails suppl√©mentaires (cr√©ateurs) pour acc√©l√©rer le traitement")
    parser.add_argument("--batch-size", type=int, default=10, help="Nombre d'√©l√©ments √† traiter avant une insertion group√©e en base de donn√©es (par d√©faut: 10)")
    parser.add_argument("--cache-dir", default="cache", help="Directory to store API responses in cache (default: 'cache')")
    parser.add_argument("--threads", type=int, default=1, help="Nombre de threads pour les requ√™tes API parall√®les (par d√©faut: 1)")
    parser.add_argument("--log-dir", default="logs", help="R√©pertoire pour stocker les fichiers de logs (par d√©faut: 'logs')")
    parser.add_argument("--verbose", action="store_true", help="Activer les logs d√©taill√©s")
    
    return parser.parse_args()

# Classe pour g√©rer le cache des requ√™tes API
class ApiCache:
    def __init__(self, cache_dir="cache", logger=None):
        self.cache_dir = cache_dir
        self.logger = logger
        os.makedirs(cache_dir, exist_ok=True)
        
    def get_cache_key(self, url):
        return hashlib.md5(url.encode()).hexdigest()
        
    def get_from_cache(self, url):
        cache_key = self.get_cache_key(url)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
        
        if os.path.exists(cache_file):
            # V√©rifier si le cache est plus r√©cent que 24 heures
            file_time = os.path.getmtime(cache_file)
            if time.time() - file_time < 86400:  # 24 heures en secondes
                try:
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        return json.load(f)
                except Exception:
                    return None
        return None
        
    def save_to_cache(self, url, data):
        cache_key = self.get_cache_key(url)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
        
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f)
        except Exception as e:
            if self.logger:
                self.logger.log(f"‚ö†Ô∏è Unable to save to cache: {e}", logging.WARNING)
            else:
                print(f"‚ö†Ô∏è Unable to save to cache: {e}")

# Classe pour g√©rer les requ√™tes API avec gestion de la limitation de d√©bit
class ApiClient:
    def __init__(self, cache_handler=None, logger=None):
        self.session = requests.Session()
        self.cache = cache_handler
        self.logger = logger
        self.last_request_time = 0
        self.rate_limit_delay = 1.0  # D√©lai entre chaque requ√™te en secondes
        
    def get(self, url, use_cache=True):
        # V√©rifier si nous avons la r√©ponse en cache
        if use_cache and self.cache:
            cached_data = self.cache.get_from_cache(url)
            if cached_data:
                if self.logger:
                    self.logger.log(f"üîÑ Utilisation du cache pour: {url}", logging.INFO)
                else:
                    print(f"üîÑ Utilisation du cache pour: {url}")
                return cached_data
                
        # Respecter le d√©lai entre les requ√™tes
        elapsed = time.time() - self.last_request_time
        if elapsed < self.rate_limit_delay:
            time.sleep(self.rate_limit_delay - elapsed)
            
        # Effectuer la requ√™te
        if self.logger:
            self.logger.log(f"üì° R√©cup√©ration des donn√©es depuis: {url}", logging.INFO)
        else:
            print(f"üì° R√©cup√©ration des donn√©es depuis: {url}")
        
        response = self.session.get(url)
        self.last_request_time = time.time()
        
        if not response.ok:
            raise Exception(f"Erreur lors de la r√©cup√©ration des donn√©es API. Code: {response.status_code}")
            
        data = response.json()
        
        # Sauvegarder dans le cache
        if use_cache and self.cache:
            self.cache.save_to_cache(url, data)
            
        return data

# Fonction pour connecter √† la base de donn√©es
def connect_to_database(args):
    try:
        conn = mysql.connector.connect(
            host=args.db_host,
            user=args.db_user,
            password=args.db_password,
            database=args.db_name
        )
        print(f"‚úÖ Connexion √† la base de donn√©es '{args.db_name}' √©tablie avec succ√®s")
        return conn
    except mysql.connector.Error as err:
        print(f"‚ùå Erreur de connexion √† la base de donn√©es: {err}")
        sys.exit(1)

# Fonction pour afficher les derni√®res lignes d'un fichier de logs
def display_recent_logs(log_file, lines=20):
    """Affiche les derni√®res lignes d'un fichier de logs"""
    if not os.path.exists(log_file):
        print(f"Le fichier de logs {log_file} n'existe pas.")
        return
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            # Lire toutes les lignes et prendre les derni√®res
            all_lines = f.readlines()
            recent_lines = all_lines[-lines:] if len(all_lines) > lines else all_lines
            
            for line in recent_lines:
                print(line.strip())
    except Exception as e:
        print(f"Erreur lors de la lecture du fichier de logs: {e}")

# Fonction pour obtenir le dernier fichier de logs cr√©√©
def get_latest_log_file(log_dir="logs"):
    """Retourne le chemin du fichier de logs le plus r√©cent"""
    if not os.path.exists(log_dir):
        return None
    
    log_files = [os.path.join(log_dir, f) for f in os.listdir(log_dir) if f.startswith("import_") and f.endswith(".log")]
    
    if not log_files:
        return None
    
    # Trier par date de modification (la plus r√©cente en premier)
    log_files.sort(key=os.path.getmtime, reverse=True)
    return log_files[0]

# Fonction pour r√©cup√©rer les IDs d'anime existants dans la base de donn√©es
def get_existing_anime_ids(conn):
    cursor = conn.cursor()
    cursor.execute("SELECT id FROM pages")
    result = cursor.fetchall()
    existing_ids = {row[0] for row in result}
    cursor.close()
    return existing_ids

# Fonction pour ins√©rer les animes par lots
def batch_insert_animes(conn, animes, logger):
    if not animes:
        return
        
    cursor = conn.cursor()
    
    # Construction de la requ√™te pour insertion multiple
    sql = """
    INSERT INTO pages (id, title, creator, broadcast, genres, episodes, studio, img, category, description, style, background_color, border_color, title_color, label_color, text_color)
    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
    """
    
    # Pr√©paration des donn√©es pour l'insertion par lots
    values = [(
        anime['id'],
        anime['title'],
        anime['creator'],
        anime['broadcast'],
        anime['genres'],
        anime['episodes'],
        anime['studio'],
        anime['img'],
        anime['category'],
        anime['synopsis'],
        anime['style'],
        anime['background_color'],
        anime['border_color'],
        anime['title_color'],
        anime['label_color'],
        anime['text_color']
    ) for anime in animes]
    
    # Ex√©cution de l'insertion par lots
    cursor.executemany(sql, values)
    conn.commit()
    cursor.close()
    
    logger.log(f"‚úÖ {len(animes)} anim√©s ont √©t√© ajout√©s en lot √† la base de donn√©es.")

# Fonction pour obtenir les d√©tails du cr√©ateur d'un anime
def get_anime_creator(api_client, anime_id, skip_details=False, logger=None):
    if skip_details:
        return "Not specified (skipped)"
        
    creator = "Not specified"
    
    # Obtenir les d√©tails complets de l'anime
    try:
        anime_url = f"https://api.jikan.moe/v4/anime/{anime_id}/full"
        anime_data = api_client.get(anime_url)
        
        # Chercher l'adaptation manga
        if "data" in anime_data and "relations" in anime_data["data"]:
            for relation in anime_data["data"]["relations"]:
                if "relation" in relation and relation["relation"] in ["Adaptation", "Source"]:
                    if "entry" in relation:
                        for entry in relation["entry"]:
                            if "type" in entry and entry["type"].lower() in ["manga", "light novel", "novel"]:
                                manga_id = entry["mal_id"]
                                
                                # R√©cup√©rer les d√©tails du manga pour trouver l'auteur
                                manga_url = f"https://api.jikan.moe/v4/manga/{manga_id}"
                                manga_data = api_client.get(manga_url)
                                
                                if "data" in manga_data and "authors" in manga_data["data"]:
                                    authors_array = [author["name"] for author in manga_data["data"]["authors"]]
                                    if authors_array:
                                        creator = ", ".join(authors_array)
                                        return creator
        
        # En cas de cr√©ateur non trouv√©, essayer de chercher dans le personnel
        if creator == "Not specified":
            staff_url = f"https://api.jikan.moe/v4/anime/{anime_id}/staff"
            staff_data = api_client.get(staff_url)
            
            if "data" in staff_data:
                mangaka_array = []
                for staff in staff_data["data"]:
                    if "positions" in staff and isinstance(staff["positions"], list):
                        for position in staff["positions"]:
                            if any(keyword in position.lower() for keyword in ["original", "creator", "story", "mangaka"]):
                                if "person" in staff and "name" in staff["person"]:
                                    mangaka_array.append(staff["person"]["name"])
                
                if mangaka_array:
                    creator = ", ".join(mangaka_array)
                    
    except Exception as e:
        if logger:
            logger.log(f"‚ö†Ô∏è Erreur lors de la r√©cup√©ration des d√©tails du cr√©ateur: {e}", logging.WARNING)
        
    return creator

# Fonction pour pr√©parer un anime pour l'insertion
def prepare_anime_data(anime, category, creator, existing_ids):
    id = anime["mal_id"]
    
    # V√©rifier si l'anime existe d√©j√† dans la base de donn√©es
    if id in existing_ids:
        return None
        
    title = anime.get("title_english", anime["title"])  # Titre anglais ou titre original
    if title is None or title == "":
        title = anime.get("title", "Unknown")
        if title is None or title == "":
            title = "Unknown"
    
    # Extraire les genres
    genres = "Not specified"
    if "genres" in anime and isinstance(anime["genres"], list):
        genres_array = [genre["name"] for genre in anime["genres"]]
        genres = ", ".join(genres_array) if genres_array else "Not specified"
    
    broadcast = anime.get("broadcast", {}).get("string", "Unknown")
    if broadcast is None or broadcast == "":
        broadcast = "Unknown"
        
    episodes = anime.get("episodes", 0)
    if episodes is None or not isinstance(episodes, int) or episodes < 0:
        episodes = 0
        
    img = anime.get("images", {}).get("jpg", {}).get("image_url", "")
    synopsis = anime.get("synopsis", "No description available.")
    if not synopsis:
        synopsis = "No description available."
    
    # Extraire le studio
    studio = "Not specified"
    if "studios" in anime and isinstance(anime["studios"], list):
        studio_array = [studio["name"] for studio in anime["studios"]]
        studio = ", ".join(studio_array) if studio_array else "Not specified"
    
    # Styles CSS pour l'affichage de l'anime
    background_color = "#252525"
    border_color = "#333333"
    title_color = "#ffffff"
    label_color = "#ffffff"
    text_color = "#ffffff"
    
    style = f"""
        .img-infos {{
            display: flex;
            flex-direction: row;
            height: 40%;
            width: 100%;
            padding: 10px 0;
            background-color: {background_color};
        }}
        .img {{
            display: flex;
            justify-content: space-around;
            width: 30%;
            height: 100%;
        }}
        .img > img {{
            height: 100%;
            border-radius: 10px;
        }}
        .infos {{
            width: 70%;
            height: 100%;
            border-left: 1px solid {border_color};
            padding-left: 10px;
        }}
        .infos h2 {{
            color: {title_color};
        }}
        .infos strong {{
            color: {label_color};
        }}
        .infos li {{
            color: {text_color};
        }}
        .description {{
            color: {text_color};
            height: 60%;
            width: 100%;
            padding: 10px 0;
            background-color: {background_color};
        }}
    """
    
    return {
        'id': id,
        'title': title,
        'creator': creator,
        'broadcast': broadcast,
        'genres': genres,
        'episodes': episodes,
        'studio': studio,
        'img': img,
        'synopsis': synopsis,
        'category': category,
        'style': style,
        'background_color': background_color,
        'border_color': border_color,
        'title_color': title_color,
        'label_color': label_color,
        'text_color': text_color
    }

# Fonction pour ins√©rer un anime avec logique de r√©essai
def process_anime_page(url, category, conn, api_client, existing_ids, logger, filter_adult=True, skip_details=False, batch_size=10):
    max_retries = 3
    attempts = 0
    success = False
    has_data = False
    
    # Liste des genres √† filtrer si filter_adult est activ√©
    adult_genres = ["Ecchi", "Erotica", "Hentai"]
    
    # Tableau pour collecter les animes √† ins√©rer en lot
    anime_batch = []
    
    # R√©cup√©rer les IDs existants avant chaque page pour √©viter les doublons
    existing_ids_set = get_existing_anime_ids(conn)

    while attempts < max_retries and not success:
        attempts += 1

        try:
            # R√©cup√©rer les donn√©es de la page d'anime
            data = api_client.get(url)
            
            if "data" not in data or not data["data"]:
                raise Exception("Aucune donn√©e trouv√©e.")
                
            # V√©rifier si nous avons atteint la derni√®re page
            if len(data["data"]) == 0:
                logger.log("üèÅ Derni√®re page atteinte, aucun anime suppl√©mentaire disponible.", logging.INFO)
                return {'success': True, 'continue_next': False, 'has_data': False}

            has_data = True
            items_processed = 0
            
            # Traiter chaque anime de la page
            for anime in data["data"]:
                id = anime["mal_id"]
                
                # V√©rifier si l'anime existe d√©j√† - utiliser le set mis √† jour
                if id in existing_ids_set:
                    logger.log(f"‚ö†Ô∏è L'anime ID: {id} existe d√©j√† dans la base de donn√©es. Ignorer...", logging.INFO)
                    items_processed += 1
                    continue
                
                title = anime.get("title_english", anime["title"])  # Titre anglais ou titre original
                if title is None or title == "":
                    title = anime.get("title", "Unknown")
                
                # Extraire les genres
                genres = "Not specified"
                genres_array = []
                if "genres" in anime and isinstance(anime["genres"], list):
                    genres_array = [genre["name"] for genre in anime["genres"] if "name" in genre]
                    genres = ", ".join(genres_array) if genres_array else "Not specified"
                
                # V√©rifier si l'anime doit √™tre filtr√© sur la base des genres pour adultes
                if filter_adult and any(adult_genre in genres for adult_genre in adult_genres):
                    logger.log(f"‚õî L'anime '{title}' ignor√© en raison du contenu pour adultes: {genres}", logging.INFO)
                    items_processed += 1
                    continue
                
                # Obtenir le cr√©ateur de l'anime
                creator = get_anime_creator(api_client, id, skip_details, logger)
                
                # Pr√©parer les donn√©es pour l'insertion
                anime_data = prepare_anime_data(anime, category, creator, existing_ids_set)
                
                if anime_data:
                    # Ajouter l'ID √† notre ensemble pour √©viter les doublons m√™me au sein du m√™me lot
                    existing_ids_set.add(id)
                    
                    anime_batch.append(anime_data)
                    logger.log(f"üîπ L'anime '{title}' pr√©par√© pour l'insertion.", logging.INFO)
                    
                    # Si nous avons atteint la taille du lot, ins√©rer les animes
                    if len(anime_batch) >= batch_size:
                        try:
                            batch_insert_animes(conn, anime_batch, logger)
                            anime_batch = []
                        except mysql.connector.Error as err:
                            if err.errno == 1062:  # Code d'erreur pour duplicate key
                                logger.log(f"‚ö†Ô∏è Erreur d'insertion en lot: {err}", logging.WARNING)
                                # Ins√©rer un par un pour identifier les probl√®mes
                                for a in anime_batch:
                                    try:
                                        batch_insert_animes(conn, [a], logger)
                                    except mysql.connector.Error as err2:
                                        logger.log(f"‚ö†Ô∏è Impossible d'ins√©rer l'anime ID {a['id']}: {err2}", logging.ERROR)
                                anime_batch = []
                            else:
                                raise
                
                items_processed += 1
            
            # Ins√©rer les animes restants
            if anime_batch:
                try:
                    batch_insert_animes(conn, anime_batch, logger)
                except mysql.connector.Error as err:
                    if err.errno == 1062:  # Code d'erreur pour duplicate key
                        logger.log(f"‚ö†Ô∏è Erreur d'insertion en lot: {err}", logging.WARNING)
                        # Ins√©rer un par un pour identifier les probl√®mes
                        for a in anime_batch:
                            try:
                                batch_insert_animes(conn, [a], logger)
                            except mysql.connector.Error as err2:
                                logger.log(f"‚ö†Ô∏è Impossible d'ins√©rer l'anime ID {a['id']}: {err2}", logging.ERROR)
                    else:
                        raise
            
            # Si on a trait√© tous les items sans erreur, marquer comme un succ√®s
            if items_processed > 0:
                success = True
                return {'success': True, 'continue_next': True, 'has_data': True}
            else:
                logger.log("üèÅ Aucun anime √† traiter dans cette page.", logging.INFO)
                return {'success': True, 'continue_next': False, 'has_data': False}

        except Exception as e:
            logger.log(f"‚ùå Tentative {attempts} √©chou√©e: {e}", logging.ERROR)
            # Attendre avant de r√©essayer
            time.sleep(5)

    # M√™me apr√®s 3 tentatives √©chou√©es, on continue avec la page suivante
    logger.log(f"‚ö†Ô∏è √âchec du traitement de la page apr√®s {max_retries} tentatives. Passage √† la page suivante...", logging.WARNING)
    return {'success': False, 'continue_next': True, 'has_data': has_data}

def process_page(page, conn, api_client, existing_ids, logger, filter_adult=True, skip_details=False, batch_size=10):
    logger.log(f"\nüîÑ Traitement de la page {page}", logging.INFO)
    
    # Variable pour suivre si on peut continuer avec la page suivante
    continue_next_page = True
    
    # Dictionnaire pour suivre les √©checs cons√©cutifs par cat√©gorie
    category_failures = {
        "trending": 0,
        "upcoming": 0,
        "general": 0
    }
    
    # üî• R√©cup√©rer les anime tendance
    if continue_next_page:
        logger.log("\nüî• Cat√©gorie: TRENDING", logging.INFO)
        result = process_anime_page(
            f"https://api.jikan.moe/v4/top/anime?page={page}",
            "trending",
            conn,
            api_client,
            existing_ids,
            logger,
            filter_adult,
            skip_details,
            batch_size
        )
        if not result['success'] and result['has_data']:
            category_failures["trending"] += 1
        continue_next_page = result['continue_next']
    
    # üìÖ R√©cup√©rer les anime √† venir
    if continue_next_page and category_failures["upcoming"] < 3:
        logger.log("\nüìÖ Cat√©gorie: UPCOMING", logging.INFO)
        result = process_anime_page(
            f"https://api.jikan.moe/v4/seasons/upcoming?page={page}",
            "upcoming",
            conn,
            api_client,
            existing_ids,
            logger,
            filter_adult,
            skip_details,
            batch_size
        )
        if not result['success'] and result['has_data']:
            category_failures["upcoming"] += 1
            if category_failures["upcoming"] >= 3:
                logger.log("\n‚ö†Ô∏è La cat√©gorie UPCOMING a √©chou√© 3 fois cons√©cutives. Cette cat√©gorie sera ignor√©e pour les pages suivantes.", logging.WARNING)
        continue_next_page = result['continue_next']
    
    # üîç R√©cup√©rer les anime g√©n√©raux (non cat√©goris√©s)
    if continue_next_page:
        logger.log("\nüîç Cat√©gorie: GENERAL", logging.INFO)
        result = process_anime_page(
            f"https://api.jikan.moe/v4/anime?page={page}",
            "none",
            conn,
            api_client,
            existing_ids,
            logger,
            filter_adult,
            skip_details,
            batch_size
        )
        if not result['success'] and result['has_data']:
            category_failures["general"] += 1
        continue_next_page = result['continue_next']
    
    return continue_next_page

def main():
    # Analyser les arguments
    args = parse_arguments()
    
    # Initialiser le logger
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logger = Logger(log_dir=args.log_dir, log_level=log_level)
    
    # Timestamp pour mesurer la dur√©e d'ex√©cution
    start_time = time.time()
    
    # G√©rer les param√®tres contradictoires
    if args.filter_adult and args.no_filter:
        logger.log("‚ö†Ô∏è Options contradictoires: --filter-adult et --no-filter. Le filtre adulte sera d√©sactiv√©.", logging.WARNING)
        args.filter_adult = False
    
    # √âtablir la connexion √† la base de donn√©es
    conn = connect_to_database(args)
    
    # R√©cup√©rer les IDs des animes existants dans la base de donn√©es
    existing_ids = get_existing_anime_ids(conn)
    logger.log(f"‚ÑπÔ∏è {len(existing_ids)} anim√©s d√©j√† pr√©sents dans la base de donn√©es.", logging.INFO)
    
    # Initialiser le gestionnaire de cache
    cache = ApiCache(args.cache_dir, logger)
    
    # Initialiser le client API
    api_client = ApiClient(cache, logger)
    
    # D√©finir la plage de pages
    start_page = args.start
    end_page = args.end
    
    # Afficher la configuration
    logger.log("\nüìã CONFIGURATION:", logging.INFO)
    logger.log(f"  - Page de d√©part: {start_page}", logging.INFO)
    logger.log(f"  - Page de fin: {('Non sp√©cifi√©e (va jusqu\'√† la fin)' if end_page is None else end_page)}", logging.INFO)
    logger.log(f"  - Filtre contenu adulte: {('Activ√©' if args.filter_adult else 'D√©sactiv√©')}", logging.INFO)
    logger.log(f"  - Recherche d√©taill√©e des cr√©ateurs: {('D√©sactiv√©e' if args.skip_details else 'Activ√©e')}", logging.INFO)
    logger.log(f"  - Taille des lots d'insertion: {args.batch_size}", logging.INFO)
    logger.log(f"  - Cache des requ√™tes API: {args.cache_dir}", logging.INFO)
    logger.log("\nüöÄ D√©marrage de l'ajout des anim√©s...\n", logging.INFO)
    
    # Compteurs pour les statistiques
    page_count = 0
    current_page = start_page
    
    try:
        # Traiter les pages jusqu'√† la fin ou jusqu'√† la page sp√©cifi√©e
        while True:
            # Traiter la page actuelle
            continue_next = process_page(
                current_page,
                conn,
                api_client,
                existing_ids,
                logger,
                args.filter_adult,
                args.skip_details,
                args.batch_size
            )
            page_count += 1
            
            # Mettre √† jour les IDs existants pour √©viter les doublons
            existing_ids = get_existing_anime_ids(conn)
            
            # V√©rifier si on doit s'arr√™ter
            if not continue_next:
                logger.log(f"\nüèÅ Arr√™t: pas plus de donn√©es disponibles apr√®s la page {current_page}", logging.INFO)
                break
                
            if end_page is not None and current_page >= end_page:
                logger.log(f"\nüèÅ Arr√™t: atteinte de la page finale sp√©cifi√©e ({end_page})", logging.INFO)
                break
                
            # Passer √† la page suivante
            current_page += 1
            
    except KeyboardInterrupt:
        logger.log("\n‚ö†Ô∏è Op√©ration interrompue par l'utilisateur", logging.WARNING)
    except Exception as e:
        logger.log(f"\n‚ùå Erreur inattendue: {e}", logging.ERROR)
    finally:
        # Calculer la dur√©e d'ex√©cution
        execution_time = time.time() - start_time
        
        # Fermer la connexion √† la base de donn√©es
        if conn.is_connected():
            conn.close()
            logger.log("\nüìä R√âSUM√â:", logging.INFO)
            logger.log(f"  - Pages trait√©es: {page_count}", logging.INFO)
            logger.log(f"  - Derni√®re page trait√©e: {current_page}", logging.INFO)
            logger.log(f"  - Dur√©e d'ex√©cution: {execution_time:.2f} secondes", logging.INFO)
            logger.log("\n‚úÖ Connexion √† la base de donn√©es ferm√©e", logging.INFO)

if __name__ == "__main__":
    main()
